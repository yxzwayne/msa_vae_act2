{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original code was written by\n",
    "# https://github.com/S-Hauri/MSA_Transformer_Generator/blob/main/MSA_Transformer_sequence_generation.py\n",
    "\n",
    "# @article{mcgee2020generative,\n",
    "#   title={Generative Capacity of Probabilistic Protein Sequence Models},\n",
    "#   author={McGee, Francisco and Novinger, Quentin and Levy, Ronald M and Carnevale, Vincenzo and Haldane, Allan},\n",
    "#   journal={arXiv preprint arXiv:2012.02296},\n",
    "#   year={2020}\n",
    "# }\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import itertools\n",
    "from typing import List, Tuple\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import esm\n",
    "import torch\n",
    "\n",
    "file_name = \"seq/6M.exper_10k.seqs\"\n",
    "save_name = \"esm_trial_gen\"\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# This is an efficient way to delete lowercase characters and insertion characters from a string\n",
    "deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "deletekeys[\".\"] = None\n",
    "deletekeys[\"*\"] = None\n",
    "translation = str.maketrans(deletekeys)\n",
    "\n",
    "\n",
    "# python esm_generator.py seq/6M.exper.seqs gen_esm_msa 629257 4 256 32\n",
    "def read_sequence(filename: str) -> Tuple[str, str]:\n",
    "    \"\"\"Reads the first (reference) sequences from a fasta or MSA file.\"\"\"\n",
    "    record = next(SeqIO.parse(filename, \"fasta\"))\n",
    "    return record.description, str(record.seq)\n",
    "\n",
    "\n",
    "def remove_insertions(sequence: str) -> str:\n",
    "    \"\"\"Removes any insertions into the sequence. Needed to load aligned sequences in an MSA.\"\"\"\n",
    "    return sequence.translate(translation)\n",
    "\n",
    "\n",
    "def read_msa(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"\n",
    "    return [\n",
    "        (record.description, remove_insertions(str(record.seq)))\n",
    "        for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)\n",
    "    ]\n",
    "\n",
    "\n",
    "def loaded_msa(msa, nseq) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Reads the nseq sequences at constant intervals from an MSA file, automatically removes insertions.\"\"\"\n",
    "    N = len(msa)\n",
    "    # split into chunks of approximately equal lengths (https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length)\n",
    "    splits = np.array_split(range(1, N), nseq)\n",
    "    output = []\n",
    "    for spl in splits:\n",
    "        record = msa[spl[0]]\n",
    "        output.append((record[\"description\"], remove_insertions(str(record[\"seq\"]))))\n",
    "    return output\n",
    "\n",
    "\n",
    "def loaded_msa_all(msa) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Reads the nseq sequences at constant intervals from an MSA file, automatically removes insertions.\"\"\"\n",
    "    N = len(msa)\n",
    "    return [\n",
    "        (record[\"description\"], remove_insertions(str(record[\"seq\"])))\n",
    "        for record in itertools.islice(msa, 1, N)\n",
    "    ]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/57237596/how-to-improve-np-random-choice-looping-efficiency\n",
    "def vectorized_choice(p, n, items=None):\n",
    "    s = p.cumsum(axis=1)\n",
    "    r = np.random.rand(p.shape[0], n, 1)\n",
    "    q = np.expand_dims(s, 1) >= r\n",
    "    k = q.argmax(axis=-1)\n",
    "    if items is not None:\n",
    "        k = np.asarray(items)[k]\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" prepare Protein data \"\"\"\n",
    "Lines = open(file_name, \"r\").readlines()\n",
    "seqs = [l.replace(\"\\n\", \"\") for l in Lines if len(l) > 3]\n",
    "msa = [{\"description\": \"noname\", \"seq\": seq} for seq in seqs]\n",
    "msa_data = loaded_msa_all(msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M, total sequences = 10000\n",
      "L, length of each sequence = 99\n",
      "A, length of alphabet = 4\n"
     ]
    }
   ],
   "source": [
    "# alphabet = \"ACDEFGHIKLMNPQRSTVWY-\"\n",
    "alphabet = \"ABCD\"\n",
    "alph_dict = {}\n",
    "for i, a in enumerate(alphabet):\n",
    "    alph_dict[a] = i\n",
    "\n",
    "M = len(seqs)\n",
    "L = len(seqs[0])\n",
    "A = len(alphabet)\n",
    "print(f\"M, total sequences = {M}\")\n",
    "print(f\"L, length of each sequence = {L}\")\n",
    "print(f\"A, length of alphabet = {A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = np.zeros((M, L, A))\n",
    "for m in range(M):\n",
    "    for i in range(L):\n",
    "        one_hot[m, i, alph_dict[seqs[m][i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 99, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = one_hot.sum(0)\n",
    "indep = counts / counts.sum(-1).reshape((-1, 1))\n",
    "entropy_per_pos = (-indep * np.log(indep + 1e-9)).sum(-1)\n",
    "pos_seq = (entropy_per_pos.argsort() + 1).tolist()  # +1 to correct for start token\n",
    "\n",
    "\n",
    "msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "# msa_transformer = msa_transformer.eval().cuda()\n",
    "msa_batch_converter = msa_alphabet.get_batch_converter()\n",
    "\n",
    "\n",
    "standard_idx = [msa_alphabet.get_idx(tok) for tok in alphabet]\n",
    "L = len(msa_data[0][1]) + 1  # add start token\n",
    "A = len(standard_idx)\n",
    "new_seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 25, 23, 13]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_transformer = msa_transformer.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 25, 23, 23, 13, 13, 13, 23, 13, 23, 23, 13,  5, 25, 23, 13, 13,  5,\n",
      "        25, 23, 13,  5, 25, 23,  5, 25, 13, 25,  5, 23, 25, 13, 23,  5,  5, 13,\n",
      "        13, 23, 25, 23,  5, 23,  5, 23, 25, 23, 23,  5, 25, 23,  5, 23, 25, 23,\n",
      "        23, 23, 23, 23, 25, 13, 25,  5, 13, 13, 23, 25, 25, 23, 25, 23, 25,  5,\n",
      "        23,  5,  5, 23,  5, 23,  5,  5, 23, 23, 23, 25,  5, 25,  5, 13, 25, 23,\n",
      "        23,  5,  5, 25, 25, 25, 23, 23, 25, 25], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_generate = 200\n",
    "n_stack = 4\n",
    "n_batch = 128\n",
    "n_mask = 66\n",
    "save_interval = 2\n",
    "\n",
    "for i in tqdm(range(0, n_generate, n_mask * n_stack)):\n",
    "    msa_batch_data = []\n",
    "    for s in range(n_stack):\n",
    "        # Randomly sample one batch worth of indices, used to pull sequences\n",
    "        idxs = random.sample(range(len(msa_data)), n_batch)\n",
    "        msa_batch_data.append([msa_data[i] for i in idxs])\n",
    "\n",
    "    msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(\n",
    "        msa_batch_data\n",
    "    )\n",
    "    msa_batch_tokens = msa_batch_tokens.to(mps_device)\n",
    "    new_tokens = msa_batch_tokens.clone()\n",
    "    \n",
    "    # mask certain proteins entirely (except start token)\n",
    "    prot_idxs = np.random.randint(n_batch, size=n_mask)\n",
    "    new_tokens[:, prot_idxs, 1:] = msa_alphabet.mask_idx\n",
    "    pprint(new_tokens[2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = msa_transformer(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10, 111, 104,  48,  55,  17,  74,  89,  25,  18, 105,  14,  34,\n",
       "        68,  56,  75, 106,  14,  34,   9, 127,  43,  31,  88,  75,   2,\n",
       "       101,  33,  81,   5,  69, 127,  59,  96,  27,  26,  74,  17,  98,\n",
       "        83,  30,  45,  22,  80, 105, 114,  99,  62,  90,   5,  63,   9,\n",
       "        83,  89,  46,  95, 109,  89,  42, 102,  39,  44,  20,  11,  51,\n",
       "       111])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, n_generate, n_mask * n_stack)):\n",
    "    msa_batch_data = []\n",
    "    for s in range(n_stack):\n",
    "        idxs = random.sample(range(len(msa_data)), n_batch)\n",
    "        msa_batch_data.append([msa_data[i] for i in idxs])\n",
    "\n",
    "    msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(\n",
    "        msa_batch_data\n",
    "    )\n",
    "    msa_batch_tokens = msa_batch_tokens.to(mps_device)\n",
    "\n",
    "    new_tokens = msa_batch_tokens.clone()\n",
    "\n",
    "    prot_idxs = np.random.randint(n_batch, size=n_mask)\n",
    "    new_tokens[\n",
    "        :, prot_idxs, 1:\n",
    "    ] = msa_alphabet.mask_idx  # mask certain proteins entirely (except start token)\n",
    "    \n",
    "    # generate {batch_size} samples, one position at a time\n",
    "    for pos in pos_seq:\n",
    "        # run model and gather masked probabilities\n",
    "        output = msa_transformer(new_tokens)\n",
    "        probs = (\n",
    "            torch.softmax(output[\"logits\"][:, prot_idxs][:, :, pos, standard_idx], -1)\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        # sample random tokens based on predicted probabilities (Gibbs sampling)\n",
    "        rand_res = vectorized_choice(probs.reshape((n_stack * n_mask, A)), 1).flatten()\n",
    "        toks = [standard_idx[t] for t in rand_res]\n",
    "        toks = torch.tensor(toks).reshape((n_stack, n_mask)).cuda()\n",
    "        # replace mask with samples\n",
    "        idxs_scat = torch.tensor(prot_idxs, dtype=int).cuda().expand(n_stack, -1)\n",
    "        new_tokens[:, :, pos].scatter_(1, idxs_scat, toks)\n",
    "    new_tokens = new_tokens.detach().cpu().numpy()\n",
    "    new_seq.append(\n",
    "        new_tokens[:, prot_idxs, 1:].reshape((-1, L - 1))\n",
    "    )  # drop start token\n",
    "\n",
    "    if len(new_seq) * n_stack * n_mask > save_interval:\n",
    "        new_seq = np.concatenate(new_seq)\n",
    "        new_strs = []\n",
    "        for seq in new_seq:\n",
    "            chars = [msa_alphabet.get_tok(idx) for idx in seq]\n",
    "            new_strs.append(\"\".join(chars))\n",
    "\n",
    "        with open(save_name + \".txt\", \"a\") as file_handler:\n",
    "            for item in new_strs:\n",
    "                file_handler.write(\"{}\\n\".format(item))\n",
    "        new_seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seq = np.concatenate(new_seq)\n",
    "new_strs = []\n",
    "for seq in new_seq:\n",
    "    chars = [msa_alphabet.get_tok(idx) for idx in seq]\n",
    "    new_strs.append(\"\".join(chars))\n",
    "\n",
    "with open(save_name + \".txt\", \"a\") as file_handler:\n",
    "    for item in new_strs:\n",
    "        file_handler.write(\"{}\\n\".format(item))\n",
    "new_seq = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
